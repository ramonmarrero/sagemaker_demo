{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import sagemaker\n",
    "from time import gmtime, strftime\n",
    "\n",
    "sagemaker_logger = logging.getLogger(\"sagemaker\")\n",
    "sagemaker_logger.setLevel(logging.INFO)\n",
    "sagemaker_logger.addHandler(logging.StreamHandler())\n",
    "\n",
    "sagemaker_session = sagemaker.Session()\n",
    "bucket = sagemaker_session.default_bucket()\n",
    "role = sagemaker.get_execution_role()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting spark_bigquery_to_s3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile spark_bigquery_to_s3.py\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from google.cloud import bigquery\n",
    "from google.oauth2 import service_account\n",
    "\n",
    "\n",
    "def main():\n",
    "    spark = SparkSession \\\n",
    "        .builder \\\n",
    "        .appName(\"Spark Demo - Writing to S3\") \\\n",
    "        .getOrCreate()\n",
    "\n",
    "    credentials = service_account.Credentials.from_service_account_file('/opt/ml/processing/input/files/credentials.json')\n",
    "\n",
    "    project_id = '<projectid>'\n",
    "    client = bigquery.Client(credentials= credentials,project=project_id)\n",
    "\n",
    "    query = \"\"\"\n",
    "       SELECT   name,  count FROM   `babynames.names_2014` WHERE   gender = 'M' ORDER BY   count DESC LIMIT   5 \"\"\"\n",
    "\n",
    "    df = client.query(query).to_dataframe()\n",
    "\n",
    "    sparkDF=spark.createDataFrame(df) \n",
    "    sparkDF.printSchema()\n",
    "    sparkDF.show()\n",
    "\n",
    "    sparkDF.repartition(1).write.mode('overwrite').\\\n",
    "        parquet('<s3 uri>')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying dependency from local path demo_credentials.json to tmpdir /tmp/tmpz1_2ab75\n",
      "Uploading dependencies from tmpdir /tmp/tmpz1_2ab75 to S3 s3://sagemaker-eu-central-1-310766717595/sm-spark-2021-04-19-09-31-19-713/input/files\n",
      "Creating processing-job with name sm-spark-2021-04-19-09-31-19-713\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  sm-spark-2021-04-19-09-31-19-713\n",
      "Inputs:  [{'InputName': 'files', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-central-1-310766717595/sm-spark-2021-04-19-09-31-19-713/input/files', 'LocalPath': '/opt/ml/processing/input/files', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}, {'InputName': 'code', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-eu-central-1-310766717595/sm-spark-2021-04-19-09-31-19-713/input/code/spark_bigquery_to_s3.py', 'LocalPath': '/opt/ml/processing/input/code', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'output-1', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-eu-central-1-310766717595/sagemaker/spark-demo/spark_event_logs', 'LocalPath': '/opt/ml/processing/spark-events/', 'S3UploadMode': 'Continuous'}}]\n",
      ".................................................................!"
     ]
    }
   ],
   "source": [
    "from sagemaker.spark.processing import PySparkProcessor\n",
    "\n",
    "prefix = \"sagemaker/spark-demo\"\n",
    "image_uri_custom = '<ecr_image_uri>'\n",
    "\n",
    "# Run the processing job\n",
    "spark_processor = PySparkProcessor(\n",
    "    base_job_name=\"sm-spark\",\n",
    "    framework_version=\"2.4\",\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type=\"ml.m5.xlarge\",\n",
    "    max_runtime_in_seconds=1200,\n",
    "    image_uri= image_uri_custom\n",
    ")\n",
    "\n",
    "spark_processor.run(\n",
    "    submit_app=\"spark_bigquery_to_s3.py\",\n",
    "    submit_files=[\"credentials.json\"],\n",
    "    spark_event_logs_s3_uri=\"s3://{}/{}/spark_event_logs\".format(bucket, prefix),\n",
    "    logs=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:eu-central-1:936697816551:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
